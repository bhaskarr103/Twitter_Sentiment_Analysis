{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary V: ['because', 'learning', 'nlp', 'happy', 'sad', 'i', 'not', 'am']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "corpus = [\n",
    "    (\"I am happy because I am learning NLP\", 1),\n",
    "    (\"I am happy\", 1),\n",
    "    ('\"I am sad, I am not learning NLP\"', 0),\n",
    "    (\"I am sad\", 0)\n",
    "]\n",
    "\n",
    "# Tokenization and extracting unique words\n",
    "vocabulary = set()\n",
    "for text, _ in corpus:  # Changed to unpack only the text, ignoring the label\n",
    "    # Tokenize text using regular expression\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Access text and then apply lower()\n",
    "    vocabulary.update(words)\n",
    "\n",
    "# Convert set to list for easier indexing\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "print(\"Vocabulary V:\", vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Tweet Corpus:\n",
      "I am happy because I am learning NLP\n",
      "I am happy\n",
      "\n",
      "Negative Tweet Corpus:\n",
      "\"I am sad, I am not learning NLP\"\n",
      "I am sad\n"
     ]
    }
   ],
   "source": [
    "positive_corpus = []\n",
    "negative_corpus = []\n",
    "\n",
    "for text, label in corpus:\n",
    "    if label == 1:\n",
    "        positive_corpus.append(text)\n",
    "    else:\n",
    "        negative_corpus.append(text)\n",
    "\n",
    "print(\"Positive Tweet Corpus:\")\n",
    "for tweet in positive_corpus:\n",
    "    print(tweet)\n",
    "\n",
    "print(\"\\nNegative Tweet Corpus:\")\n",
    "for tweet in negative_corpus:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Frequency:\n",
      "{'because': 1, 'learning': 1, 'nlp': 1, 'happy': 2, 'sad': 0, 'i': 3, 'not': 0, 'am': 3}\n",
      "\n",
      "Negative Frequency:\n",
      "{'because': 0, 'learning': 1, 'nlp': 1, 'happy': 0, 'sad': 2, 'i': 3, 'not': 1, 'am': 3}\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary from previous step\n",
    "\n",
    "\n",
    "positive_frequency = {word: 0 for word in vocabulary}\n",
    "negative_frequency = {word: 0 for word in vocabulary}\n",
    "\n",
    "# Counting positive word frequencies\n",
    "for tweet in positive_corpus:\n",
    "    words = re.findall(r'\\b\\w+\\b', tweet.lower())\n",
    "    for word in words:\n",
    "        if word in positive_frequency:\n",
    "            positive_frequency[word] += 1\n",
    "\n",
    "# Counting negative word frequencies\n",
    "for tweet in negative_corpus:\n",
    "    words = re.findall(r'\\b\\w+\\b', tweet.lower())\n",
    "    for word in words:\n",
    "        if word in negative_frequency:\n",
    "            negative_frequency[word] += 1\n",
    "\n",
    "print(\"Positive Frequency:\")\n",
    "print(positive_frequency)\n",
    "print(\"\\nNegative Frequency:\")\n",
    "print(negative_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Vector: [1, 17, 14]\n"
     ]
    }
   ],
   "source": [
    "def encode_tweet(tweet, positive_freq, negative_freq):\n",
    "    words = re.findall(r'\\b\\w+\\b', tweet.lower())\n",
    "    positive_sum = sum(positive_freq.get(word, 0) for word in words)\n",
    "    negative_sum = sum(negative_freq.get(word, 0) for word in words)\n",
    "    return [1, positive_sum, negative_sum]\n",
    "\n",
    "# Example usage:\n",
    "tweet = \"I am happy because I am learning NLP\"\n",
    "encoded_vector = encode_tweet(tweet, positive_frequency, negative_frequency)\n",
    "print(\"Encoded Vector:\", encoded_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix X:\n",
      "[[ 1. 17. 10.]\n",
      " [ 1. 14. 12.]\n",
      " [ 1.  8.  4.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_freqs(tweets, labels):\n",
    "    freqs = {}\n",
    "    for tweet, label in zip(tweets, labels):\n",
    "        words = re.findall(r'\\b\\w+\\b', tweet.lower())\n",
    "        for word in words:\n",
    "            pair = (word, label)\n",
    "            freqs[pair] = freqs.get(pair, 0) + 1\n",
    "    return freqs\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    return re.findall(r'\\b\\w+\\b', tweet.lower())\n",
    "\n",
    "def extract_features(tweet, freqs):\n",
    "    positive_sum = 0\n",
    "    negative_sum = 0\n",
    "    for word in tweet:\n",
    "        positive_sum += freqs.get((word, 1), 0)\n",
    "        negative_sum += freqs.get((word, 0), 0)\n",
    "    return [1, positive_sum, negative_sum]\n",
    "\n",
    "# Example data\n",
    "tweets = [\n",
    "    \"I am happy because I am learning NLP\",\n",
    "    \"I am sad, I am not learning NLP\",\n",
    "    \"I am happy\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1]\n",
    "\n",
    "# Build frequencies dictionary\n",
    "freqs = build_freqs(tweets, labels)\n",
    "\n",
    "# Initialize matrix X\n",
    "m = len(tweets)\n",
    "X = np.zeros((m, 3))\n",
    "\n",
    "# Extract features for each tweet\n",
    "for i in range(m):\n",
    "    p_tweet = process_tweet(tweets[i])\n",
    "    X[i, :] = extract_features(p_tweet, freqs)\n",
    "\n",
    "print(\"Matrix X:\")\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized theta: [ 0.20345546  1.13460441 -1.58399181]\n",
      "Final cost: 0.043147541252030866\n",
      "Prediction: 0.0639148397755089\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cost_function(theta, X, y):\n",
    "    \"\"\"Compute the cost function.\"\"\"\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    J = -1/m * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return J\n",
    "\n",
    "def gradient_descent(theta, X, y, alpha, num_iterations):\n",
    "    \"\"\"Perform gradient descent to optimize theta.\"\"\"\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        h = sigmoid(np.dot(X, theta))\n",
    "        gradient = np.dot(X.T, (h - y)) / m\n",
    "        theta -= alpha * gradient\n",
    "        J_history.append(cost_function(theta, X, y))\n",
    "\n",
    "    return theta, J_history\n",
    "\n",
    "# Example usage:\n",
    "# Initialize theta with zeros\n",
    "theta = np.zeros(X.shape[1])\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# Define labels\n",
    "y = np.array(labels)\n",
    "\n",
    "# Perform gradient descent\n",
    "theta_optimized, J_history = gradient_descent(theta, X, y, alpha, num_iterations)\n",
    "\n",
    "# Print optimized theta and cost history\n",
    "print(\"Optimized theta:\", theta_optimized)\n",
    "print(\"Final cost:\", J_history[-1])\n",
    "\n",
    "# You can use the optimized theta to make predictions on new data.\n",
    "# For example, to predict the label for a new tweet:\n",
    "new_tweet = \"I am happy because I am learning NLP\"\n",
    "encoded_vector = encode_tweet(new_tweet, positive_frequency, negative_frequency)\n",
    "prediction = sigmoid(np.dot(encoded_vector, theta_optimized))\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.0639148397755089\n",
      "Hypothesis (sigmoid(np.dot(X, theta_optimized))): [0.97471201 0.05117445 0.94999451]\n",
      "Parameters (theta_optimized): [ 0.20345546  1.13460441 -1.58399181]\n",
      "Features (X): [[ 1. 17. 10.]\n",
      " [ 1. 14. 12.]\n",
      " [ 1.  8.  4.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# Initialize theta with zeros\n",
    "theta = np.zeros(X.shape[1])\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# Define labels\n",
    "y = np.array(labels)\n",
    "\n",
    "# Perform gradient descent\n",
    "theta_optimized, J_history = gradient_descent(theta, X, y, alpha, num_iterations)\n",
    "\n",
    "new_tweet = \"I am happy because I am learning NLP\"\n",
    "encoded_vector = encode_tweet(new_tweet, positive_frequency, negative_frequency)\n",
    "hypothesis = sigmoid(np.dot(encoded_vector, theta_optimized))\n",
    "print(\"Prediction:\", hypothesis)\n",
    "\n",
    "# Print hypothesis, parameters, and features as arrays\n",
    "print(\"Hypothesis (sigmoid(np.dot(X, theta_optimized))):\", sigmoid(np.dot(X, theta_optimized)))\n",
    "print(\"Parameters (theta_optimized):\", theta_optimized)\n",
    "print(\"Features (X):\", X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative sentiment (label 0)\n"
     ]
    }
   ],
   "source": [
    "# Determine sentiment based on probability threshold\n",
    "threshold = 0.5\n",
    "if prediction >= threshold:\n",
    "    print(\"positive sentiment (label 1)\")\n",
    "else:\n",
    "    print(\"negative sentiment (label 0)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
